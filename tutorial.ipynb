{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101: General Concepts\n",
    "   \n",
    "By the end of this section you will\n",
    "\n",
    "1. Know how to extract features from real-world data in order to perform machine learning tasks.\n",
    "2. Know the basic categories of `supervised learning`, including `classification` and `regression` problems.\n",
    "3. Know the basic categories of `unsupervised learning`, including dimensionality reduction and clustering.\n",
    "4. Understand the distinction between linearly separable and non-linearly separable data.\n",
    "      \n",
    "In addition, you will know several tools within scikit-learn which can be used to accomplish the above tasks.\n",
    "\n",
    "In this section we will begin to explore the basic principles of\n",
    "machine learning.\n",
    "Machine Learning is about building **programs with tunable parameters**\n",
    "(typically an array of floating point values) that are adjusted\n",
    "automatically so as to improve their behavior by **adapting to\n",
    "previously seen data**.\n",
    "\n",
    "> Machine Learning can be considered a **subfield of Artificial Intelligence** since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow **generalizing** rather that just storing and retrieving data items like a database system would do.\n",
    "\n",
    "A very simple example of a machine learning task can be seen in the following\n",
    "figure: it shows a collection of two-dimensional data, colored according\n",
    "to two different class labels.  A classification algorithm is used to draw\n",
    "a dividing boundary between the two clusters of points:\n",
    "\n",
    ".. figure:: auto_examples/images/plot_sgd_separating_hyperplane_1.png\n",
    "   :target: auto_examples/plot_sgd_separating_hyperplane.html\n",
    "   :align: center\n",
    "   :scale: 80%\n",
    "\n",
    "   Example Linear Decision Boundary\n",
    "\n",
    "As with all figures in this tutorial, the above image has a hyper-link to the\n",
    "python source code which is used to generate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Features and feature extraction\n",
    "\n",
    "Most machine learning algorithms implemented in ``scikit-learn`` expect a numpy array as input ``X``. \n",
    "\n",
    "The expected shape of ``X`` is **(n_samples, n_features)**\n",
    "\n",
    "* **n_samples**: The number of samples: each sample is an item to process (e.g. classify).\n",
    "  * A sample can be a document, a picture, a sound, a video, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "\n",
    "* **``n_features``**: The number of features or distinct traits that can be used to describe each item in a quantitative manner.\n",
    "  * The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being zeros for a given sample. In this case we may use ``scipy.sparse`` matrices instead of ``numpy`` arrays so as to make the data fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 A simple example: the iris dataset\n",
    "\n",
    "![](./images/iris.jpg)\n",
    "\n",
    "The machine learning community often uses a simple flowers database where each row in the database (or CSV file) is a set of\n",
    "measurements of an individual iris flower.\n",
    "\n",
    "Each sample in this dataset is described by 4 features and can belong to one of the target classes:\n",
    "\n",
    "* **Features** in the Iris dataset:\n",
    "   0. sepal length in cm\n",
    "   1. sepal width in cm\n",
    "   2. petal length in cm\n",
    "   3. petal width in cm\n",
    "\n",
    "\n",
    "* **Target** classes to predict:\n",
    "   0. Iris Setosa\n",
    "   1. Iris Versicolour\n",
    "   2. Iris Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``scikit-learn`` embeds a copy of the iris CSV file along with a\n",
    "helper function to load it into numpy arrays::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of each sample flower are stored in the ``data`` attribute of the dataset::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150L"
      ]
     },
     "execution_count": 19,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4L"
      ]
     },
     "execution_count": 20,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.1,  3.5,  1.4,  0.2])"
      ]
     },
     "execution_count": 21,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the class of each sample is stored in the `target`` attribute of the dataset::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(iris.target) == n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the classes are stored in the last attribute, namely ``target_names``::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 24,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Handling categorical features\n",
    "\n",
    "Sometimes people describe samples with categorical descriptors that have no obvious numerical representation.\n",
    "\n",
    "For instance assume that each flower is further described by a color name among a fixed list of color names:\n",
    "\n",
    "```python\n",
    "color in ['purple', 'blue', 'red']\n",
    "```\n",
    "\n",
    "![](./images/iris_color.jpg)\n",
    "\n",
    "The simple way to turn this categorical feature into numerical\n",
    "features suitable for machine learning is to create new features\n",
    "for each distinct color name that can be valued to ``1.0`` if the\n",
    "category is matching or ``0.0`` if not.\n",
    "\n",
    "The enriched iris feature set would hence be in this case:\n",
    "\n",
    "  0. sepal length in cm\n",
    "  1. sepal width in cm\n",
    "  2. petal length in cm\n",
    "  3. petal width in cm\n",
    "  4. color#purple (1.0 or 0.0)\n",
    "  5. color#blue (1.0 or 0.0)\n",
    "  6. color#red (1.0 or 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Extracting features from unstructured data\n",
    "\n",
    "The previous example deals with features that are readily available in a structured dataset with rows and columns of numerical or categorical values.\n",
    "\n",
    "However, **most of the produced data is not readily available in a structured representation** such as SQL, CSV, XML, JSON or RDF.\n",
    "\n",
    "Here is an overview of strategies to turn unstructed data items into arrays of numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text documents\n",
    "\n",
    "Count the frequency of each word or pair of consecutive words in each document. This approach is called `Bag of Words <http://scikit-learn.org/dev/modules/feature_extraction.html#text-feature-extraction>`_\n",
    "\n",
    "*Note: we include other file formats such as HTML and PDF in this category: an ad-hoc preprocessing step is required to extract the plain text in UTF-8 encoding for instance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images\n",
    "\n",
    "- Rescale the picture to a fixed size and **take all the raw\n",
    "  pixels values** (with or without luminosity normalization)\n",
    "\n",
    "- Take some transformation of the signal (gradients in each\n",
    "  pixel, wavelets transforms...)\n",
    "\n",
    "- Compute the Euclidean, Manhattan or cosine **similarities of\n",
    "  the sample to a set reference prototype images** aranged in a\n",
    "  code book.  The code book may have been previously extracted\n",
    "  from the same dataset using an unsupervised learning algorithm\n",
    "  on the raw pixel signal.\n",
    "\n",
    "  Each feature value is the distance to one element of the code\n",
    "  book.\n",
    "\n",
    "- Perform **local feature extraction**: split the picture into\n",
    "  small regions and perform feature extraction locally in each\n",
    "  area.\n",
    "\n",
    "  Then combine all the features of the individual areas into a\n",
    "  single array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sounds\n",
    "\n",
    "Same strategy as for images within a 1D space instead of 2D\n",
    "\n",
    "Practical implementations of such feature extraction strategies will be presented in the last sections of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Supervised Learning, Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be broken into two broad regimes: `supervised learning` and `unsupervised learning`. \n",
    "\n",
    "We'll introduce these concepts here, and discuss them in more detail below.\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of both *features* and *labels*.  The task is to construct an estimator which is able to predict the label of an object given the set of features.\n",
    "A relatively simple example is predicting the species of iris given a set of measurements of its flower.  This is a relatively simple task.\n",
    "\n",
    "Some more complicated examples are:\n",
    "\n",
    "- given a multicolor image of an object through a telescope, determine\n",
    "  whether that object is a star, a quasar, or a galaxy.\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like (A famous example\n",
    "  is the `Netflix Prize <http://en.wikipedia.org/wiki/Netflix_prize>`_).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What these tasks have in common is that there is one or more unknown\n",
    "quantities associated with the object which needs to be determined\n",
    "from other observed quantities.  Supervised learning is further broken\n",
    "down into two categories, *classification* and *regression*.  In\n",
    "classification, the label is discrete, while in regression, the label\n",
    "is continuous.  For example, in astronomy, the task of determining\n",
    "whether an object is a star, a galaxy, or a quasar is a classification\n",
    "problem: the label is from three distinct categories.  On the other\n",
    "hand, we might wish to determine the age of an object based on\n",
    "such observations: this would be a regression problem: the label (age)\n",
    "is a continuous quantity.\n",
    "\n",
    "**Unsupervised Learning** addresses a different sort of problem.  Here\n",
    "the data has no labels, and we are interested in finding similarities\n",
    "between the objects in question.  In a sense, you can think of unsupervised\n",
    "learning as a means of discovering labels from the data itself.\n",
    "Unsupervised learning comprises\n",
    "tasks such as dimensionality reduction, clustering, and density estimation.\n",
    "For example, in the iris data discussed above, we can used unsupervised\n",
    "methods to determine combinations of the measurements which best \n",
    "display the structure of the data.  As we'll see below, such a projection\n",
    "of the data can be used to visualize the four-dimensional dataset in\n",
    "two dimensions.\n",
    "Some more involved unsupervised learning problems are:\n",
    "\n",
    "- given detailed observations of distant galaxies, determine which features\n",
    "  or combinations of features are most important in distinguishing between\n",
    "  galaxies.\n",
    "- given a mixture of two sound sources (for example, a person talking over\n",
    "  some music), separate the two (this is called the\n",
    "  `blind source separation <http://en.wikipedia.org/wiki/Blind_signal_separation>`_ problem).\n",
    "- given a video, isolate a moving object and categorize in relation to\n",
    "  other moving objects which have been seen.\n",
    "\n",
    "``scikit-learn`` strives to have a uniform interface across all methods,\n",
    "and we'll see examples of these below.  Given a ``scikit-learn`` estimator\n",
    "object named ``model``, the following methods are available:\n",
    "\n",
    "- **Available in all Estimators**\n",
    "\n",
    "  - ``model.fit()`` : fit training data.  For supervised learning applications,\n",
    "    this accepts two arguments: the data ``X`` and the labels ``y``\n",
    "    (e.g. ``model.fit(X, y)``).  For unsupervised learning applications,\n",
    "    this accepts only a single argument, the data ``X``\n",
    "    (e.g. ``model.fit(X)``).\n",
    "\n",
    "- **Available in supervised estimators**\n",
    "\n",
    "  - ``model.predict()`` : given a trained model, predict the label of\n",
    "    a new set of data.  This method accepts one argument, the new\n",
    "    data ``X_new`` (e.g. ``model.predict(X_new)``), and returns the\n",
    "    learned label for each object in the array.\n",
    "  - ``model.predict_proba()`` : For classification problems,\n",
    "    some estimators also provide this method, which returns the probability\n",
    "    that a new observation has each categorical label.  In this case, the\n",
    "    label with the highest probability is returned by ``model.predict()``.\n",
    "\n",
    "- **Available in unsupervised estimators**\n",
    "\n",
    "  - ``model.transform()`` : given an unsupervised model, transform new data\n",
    "    into the new basis.  This also accepts one argument ``X_new``, and\n",
    "    returns the new representation of the data based on the unsupervised\n",
    "    model.\n",
    "  - ``model.fit_transform()`` : some estimators implement this method,\n",
    "    which performs a ``fit`` and a ``transform`` on the same input data.\n",
    "\n",
    "\n",
    ".. _astro_supervised_learning:\n",
    "\n",
    "Supervised Learning: ``model.fit(X, y)``\n",
    "----------------------------------------\n",
    "\n",
    ".. figure:: auto_examples/images/plot_ML_flow_chart_1.png\n",
    "   :target: auto_examples/plot_ML_flow_chart.html\n",
    "   :scale: 75 %\n",
    "   :align: center\n",
    "   :alt: Flow diagram for supervised learning\n",
    "\n",
    ".. figure:: auto_examples/images/plot_ML_flow_chart_2.png\n",
    "   :target: auto_examples/plot_ML_flow_chart.html\n",
    "   :scale: 75 %\n",
    "   :align: center\n",
    "   :alt: Flow diagram for supervised learning with scikit-learn\n",
    "\n",
    "   Overview of supervised Learning with scikit-learn\n",
    "\n",
    "As mentioned above, a supervised learning algorithm makes the distinction\n",
    "between the raw observed data ``X`` with shape ``(n_samples, n_features)``\n",
    "and some label given to the model during training. In ``scikit-learn``\n",
    "this array is often noted ``y`` and has generally the shape ``(n_samples,)``.\n",
    "After training, the fitted model will try to predict the most likely labels\n",
    "``y_new`` for new a set of samples ``X_new``.\n",
    "\n",
    "Depending on the nature of the target ``y``, supervised learning\n",
    "can be given different names:\n",
    "\n",
    "  - If ``y`` has values in a fixed set of **categorical outcomes**\n",
    "    (represented by **integers**) the task to predict ``y`` is called\n",
    "    **classification**.\n",
    "\n",
    "  - If ``y`` has **floating point values** (e.g. to represent a price,\n",
    "    a temperature, a size...), the task to predict ``y`` is called\n",
    "    **regression**.\n",
    "\n",
    "Classification\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "Classification is the task of predicting the value of a categorical\n",
    "variable given some input variables (a.k.a. the features or \"predictors\").\n",
    "This section includes a first exploration of classification with\n",
    "scikit-learn. We'll explore a detailed example of classification with\n",
    "astronomical data in :ref:`astronomy_classification`.\n",
    "\n",
    "\n",
    "A first classifier example with ``scikit-learn``\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    ".. note::\n",
    "\n",
    "   The information in this section is available in an interactive notebook\n",
    "   :download:`02_iris_classification.ipynb <notebooks/02_iris_classification.ipynb>`,\n",
    "   which can be viewed using `iPython notebook`_.    An online static view can\n",
    "   be seen `here <http://nbviewer.ipython.org/url/astroml.github.com/sklearn_tutorial/_downloads/02_iris_classification.ipynb>`_\n",
    "\n",
    "In the iris dataset example, suppose we are assigned the task to\n",
    "guess the class of an individual flower given the measurements of\n",
    "petals and sepals. This is a classification task, hence we have::\n",
    "\n",
    "  >>> X, y = iris.data, iris.target\n",
    "\n",
    "Once the data has this format it is trivial to train a classifier,\n",
    "for instance a support vector machine with a linear kernel::\n",
    "\n",
    "  >>> from sklearn.svm import LinearSVC\n",
    "  >>> clf = LinearSVC()\n",
    "\n",
    ".. note::\n",
    "\n",
    "    Whenever you import a scikit-learn class or function for the first time,\n",
    "    you are advised to read the docstring by using the ``?`` magic suffix\n",
    "    of ipython, for instance type: ``LinearSVC?``.\n",
    "\n",
    "\n",
    "``clf`` is a statistical model that has parameters that control the\n",
    "learning algorithm (those parameters are sometimes called the\n",
    "hyperparameters). Those hyperparameters can be supplied by the\n",
    "user in the constructor of the model. We will explain later how to choose\n",
    "a good combination using either simple empirical rules or data\n",
    "driven selection::\n",
    "\n",
    "  >>> clf\n",
    "  LinearSVC(C=1.0, dual=True, fit_intercept=True, intercept_scaling=1,\n",
    "       loss='l2', multi_class=False, penalty='l2', tol=0.0001)\n",
    "\n",
    "By default the real model parameters are not initialized. They will be\n",
    "tuned automatically from the data by calling the ``fit`` method::\n",
    "\n",
    "  >>> clf = clf.fit(X, y)\n",
    "\n",
    "  >>> clf.coef_                         # doctest: +ELLIPSIS\n",
    "  array([[ 0.18...,  0.45..., -0.80..., -0.45...],\n",
    "         [ 0.05..., -0.89...,  0.40..., -0.93...],\n",
    "         [-0.85..., -0.98...,  1.38...,  1.86...]])\n",
    "\n",
    "  >>> clf.intercept_                    # doctest: +ELLIPSIS\n",
    "  array([ 0.10...,  1.67..., -1.70...])\n",
    "\n",
    "Once the model is trained, it can be used to predict the most likely outcome on\n",
    "unseen data. For instance let us define a list of simple sample that looks\n",
    "like the first sample of the iris dataset::\n",
    "\n",
    "  >>> X_new = [[ 5.0,  3.6,  1.3,  0.25]]\n",
    "\n",
    "  >>> clf.predict(X_new)\n",
    "  array([0], dtype=int32)\n",
    "\n",
    "The outcome is ``0`` which is the id of the first iris class, namely\n",
    "'setosa'.\n",
    "\n",
    "The following figure places the location of the ``fit`` and ``predict``\n",
    "calls on the previous flow diagram. The ``vec`` object is a vectorizer\n",
    "used for feature extraction that is not used in the case of the iris\n",
    "data (it already comes as vectors of features):\n",
    "\n",
    "Some ``scikit-learn`` classifiers can further predict probabilities\n",
    "of the outcome.  This is the case of logistic regression models::\n",
    "\n",
    "  >>> from sklearn.linear_model import LogisticRegression\n",
    "  >>> clf2 = LogisticRegression().fit(X, y)\n",
    "  >>> clf2\n",
    "  LogisticRegression(C=1.0, dual=False, fit_intercept=True, intercept_scaling=1,\n",
    "            penalty='l2', tol=0.0001)\n",
    "  >>> clf2.predict_proba(X_new)\n",
    "  array([[  9.07512928e-01,   9.24770379e-02,   1.00343962e-05]])\n",
    "\n",
    "This means that the model estimates that the sample in ``X_new`` has:\n",
    "\n",
    "  - 90% likelyhood to belong to the 'setosa' class\n",
    "\n",
    "  - 9% likelyhood to belong to the 'versicolor' class\n",
    "\n",
    "  - 1% likelyhood to belong to the 'virginica' class\n",
    "\n",
    "Of course, the ``predict`` method that outputs the label id of the\n",
    "most likely outcome is also available::\n",
    "\n",
    "  >>> clf2.predict(X_new)\n",
    "  array([0], dtype=int32)\n",
    "\n",
    "\n",
    "Notable implementations of classifiers\n",
    "++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    ":class:`sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "  Regularized Logistic Regression based on ``liblinear``\n",
    "\n",
    ":class:`sklearn.svm.LinearSVC`\n",
    "\n",
    "  Support Vector Machines without kernels based on ``liblinear``\n",
    "\n",
    ":class:`sklearn.svm.SVC`\n",
    "\n",
    "  Support Vector Machines with kernels based on ``libsvm``\n",
    "\n",
    ":class:`sklearn.linear_model.SGDClassifier`\n",
    "\n",
    "  Regularized linear models (SVM or logistic regression) using a Stochastic\n",
    "  Gradient Descent algorithm written in ``Cython``\n",
    "\n",
    ":class:`sklearn.neighbors.NeighborsClassifier`\n",
    "\n",
    "  k-Nearest Neighbors classifier based on the ball tree datastructure for low\n",
    "  dimensional data and brute force search for high dimensional data\n",
    "\n",
    ":class:`sklearn.naive_bayes.GaussianNB`\n",
    "\n",
    "  Gaussian Naive Bayes model.  This is an unsophisticated model which can\n",
    "  be trained very quickly.  It is often used to obtain baseline results\n",
    "  before moving to a more sophisticated classifier.\n",
    "\n",
    "\n",
    "Sample application of classifiers\n",
    "+++++++++++++++++++++++++++++++++\n",
    "\n",
    "The following table gives examples of applications of classifiers\n",
    "for some common engineering tasks:\n",
    "\n",
    "============================================ =================================\n",
    "Task                                         Predicted outcomes\n",
    "============================================ =================================\n",
    "E-mail classification                        Spam, normal, priority mail\n",
    "-------------------------------------------- ---------------------------------\n",
    "Language identification in text documents    en, es, de, fr, ja, zh, ar, ru...\n",
    "-------------------------------------------- ---------------------------------\n",
    "News articles categorization                 Business, technology, sports...\n",
    "-------------------------------------------- ---------------------------------\n",
    "Sentiment analysis in customer feedback      Negative, neutral, positive\n",
    "-------------------------------------------- ---------------------------------\n",
    "Face verification in pictures                Same / different person\n",
    "-------------------------------------------- ---------------------------------\n",
    "Speaker verification in voice recordings     Same / different person\n",
    "-------------------------------------------- ---------------------------------\n",
    "Astronomical Sources                         Object type or class\n",
    "============================================ =================================\n",
    "\n",
    "\n",
    "Regression\n",
    "~~~~~~~~~~\n",
    "\n",
    "Regression is the task of predicting the value of a continuously varying\n",
    "variable (e.g. a price, a temperature, a conversion rate...) given\n",
    "some input variables (a.k.a. the features, \"predictors\" or\n",
    "\"regressors\").  We'll explore a detailed example of regression with\n",
    "astronomical data in :ref:`astronomy_regression`.\n",
    "\n",
    "Some notable implementations of regression models in ``scikit-learn`` include:\n",
    "\n",
    ":class:`sklearn.linear_model.Ridge`\n",
    "\n",
    "  L2-regularized least squares linear model\n",
    "\n",
    ":class:`sklearn.linear_model.ElasticNet`\n",
    "\n",
    "  L1+L2-regularized least squares linear model trained using\n",
    "  Coordinate Descent\n",
    "\n",
    ":class:`sklearn.linear_model.LassoLARS`\n",
    "\n",
    "  L1-regularized least squares linear model trained with Least Angle\n",
    "  Regression\n",
    "\n",
    ":class:`sklearn.linear_model.SGDRegressor`\n",
    "\n",
    "  L1+L2-regularized least squares linear model trained using\n",
    "  Stochastic Gradient Descent\n",
    "\n",
    ":class:`sklearn.linear_model.ARDRegression`\n",
    "\n",
    "  Bayesian Automated Relevance Determination regression\n",
    "\n",
    ":class:`sklearn.svm.SVR`\n",
    "\n",
    "  Non-linear regression using Support Vector Machines (wrapper for\n",
    "  ``libsvm``)\n",
    "\n",
    ":class:`sklearn.ensemble.RandomForestRegressor`\n",
    "\n",
    "  An ensemble method which constructs multiple decision trees from subsets\n",
    "  of the data.\n",
    "\n",
    "\n",
    ".. _astro_unsupervised_learning:\n",
    "\n",
    "Unsupervised Learning: ``model.fit(X)``\n",
    "---------------------------------------\n",
    "\n",
    ".. figure:: auto_examples/images/plot_ML_flow_chart_3.png\n",
    "   :target: auto_examples/plot_ML_flow_chart.html\n",
    "   :scale: 75 %\n",
    "   :align: center\n",
    "   :alt: Flow diagram for supervised learning with scikit-learn\n",
    "\n",
    "   Unsupervised Learning overview\n",
    "\n",
    "An unsupervised learning algorithm only uses a single set of\n",
    "observations ``X`` with shape ``(n_samples, n_features)`` and does\n",
    "not use any kind of labels.\n",
    "\n",
    "An unsupervised learning model will try to fit its parameters so\n",
    "as to best summarize regularities found in the data.\n",
    "\n",
    "The following introduces the main variants of unsupervised learning\n",
    "algorithms, namely dimensionality reduction and clustering.\n",
    "\n",
    "\n",
    "Dimensionality Reduction and visualization\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Dimensionality reduction is the task of deriving a set of **new artificial\n",
    "features** that is **smaller** than the original feature set while\n",
    "retaining **most of the variance** of the original data.\n",
    "\n",
    "\n",
    "Normalization and visualization with PCA\n",
    "++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    ".. note::\n",
    "\n",
    "   The information in this section is available in an interactive notebook\n",
    "   :download:`03_iris_dimensionality.ipynb <notebooks/03_iris_dimensionality.ipynb>`,\n",
    "   which can be viewed using `iPython notebook`_.    An online static view can\n",
    "   be seen `here <http://nbviewer.ipython.org/url/astroml.github.com/sklearn_tutorial/_downloads/03_iris_dimensionality.ipynb>`_\n",
    "\n",
    "The most common technique for dimensionality reduction is called\n",
    "**Principal Component Analysis**.\n",
    "\n",
    "PCA can be done using linear combinations of the original features\n",
    "using a truncated `Singular Value Decomposition\n",
    "<http://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD>`_\n",
    "of the matrix ``X``\n",
    "so as to project the data onto a base of the top singular vectors.\n",
    "\n",
    "If the number of retained components is 2 or 3, PCA can be used to\n",
    "visualize the dataset::\n",
    "\n",
    "  >>> from sklearn.decomposition import PCA\n",
    "  >>> pca = PCA(n_components=2, whiten=True).fit(X)\n",
    "\n",
    "Once fitted, the ``pca`` model exposes the singular vectors in the\n",
    "``components_`` attribute::\n",
    "\n",
    "  >>> pca.components_                                      # doctest: +ELLIPSIS\n",
    "  array([[ 0.17..., -0.04...,  0.41...,  0.17...],\n",
    "         [-1.33..., -1.48...,  0.35...,  0.15...]])\n",
    "\n",
    "  >>> pca.explained_variance_ratio_                        # doctest: +ELLIPSIS\n",
    "  array([ 0.92...,  0.05...])\n",
    "\n",
    "  >>> pca.explained_variance_ratio_.sum()                  # doctest: +ELLIPSIS\n",
    "  0.97...\n",
    "\n",
    "Let us project the iris dataset along those first 2 dimensions::\n",
    "\n",
    "  >>> X_pca = pca.transform(X)\n",
    "\n",
    "The dataset has been \"normalized\", which means that the data is now centered on\n",
    "both components with unit variance::\n",
    "\n",
    "  >>> import numpy as np\n",
    "  >>> np.round(X_pca.mean(axis=0), decimals=5)\n",
    "  array([-0.,  0.])\n",
    "\n",
    "  >>> np.round(X_pca.std(axis=0), decimals=5)\n",
    "  array([ 1.,  1.])\n",
    "\n",
    "Furthermore the samples components do no longer carry any linear\n",
    "correlation::\n",
    "\n",
    "  >>> import numpy as np\n",
    "  >>> np.round(np.corrcoef(X_pca.T), decimals=5)\n",
    "  array([[ 1., -0.],\n",
    "         [-0.,  1.]])\n",
    "\n",
    "We can visualize the dataset using ``pylab``, for instance by defining the\n",
    "following utility function::\n",
    "\n",
    "  >>> import pylab as pl\n",
    "  >>> from itertools import cycle\n",
    "  >>> def plot_2D(data, target, target_names):\n",
    "  ...     colors = cycle('rgbcmykw')\n",
    "  ...     target_ids = range(len(target_names))\n",
    "  ...     pl.figure()\n",
    "  ...     for i, c, label in zip(target_ids, colors, target_names):\n",
    "  ...         pl.scatter(data[target == i, 0], data[target == i, 1],\n",
    "  ...                    c=c, label=label)\n",
    "  ...     pl.legend()\n",
    "  ...     pl.show()\n",
    "  ...\n",
    "\n",
    "Calling ``plot_2D(X_pca, iris.target, iris.target_names)`` will\n",
    "display the following:\n",
    "\n",
    "\n",
    ".. figure:: auto_examples/images/plot_iris_projections_1.png\n",
    "   :target: auto_examples/plot_iris_projections.html\n",
    "   :scale: 65 %\n",
    "   :align: center\n",
    "   :alt: 2D PCA projection of the iris dataset\n",
    "\n",
    "   2D PCA projection of the iris dataset\n",
    "\n",
    "Note that this projection was determined *without* any information about the\n",
    "labels (represented by the colors): this is the sense in which the learning\n",
    "is unsupervised.  Nevertheless, we see that the projection gives us insight\n",
    "into the distribution of the different flowers in parameter space: notably,\n",
    "*iris setosa* is much more distinct than the other two species.\n",
    "\n",
    "\n",
    ".. note::\n",
    "\n",
    "  The default implementation of PCA computes the SVD of the full\n",
    "  data matrix, which is not scalable when both ``n_samples`` and\n",
    "  ``n_features`` are big (more that a few thousands).\n",
    "\n",
    "  If you are interested in a number of components that is much\n",
    "  smaller than both ``n_samples`` and ``n_features``, consider using\n",
    "  :class:`sklearn.decomposition.RandomizedPCA` instead.\n",
    "\n",
    "\n",
    "Other applications of dimensionality reduction\n",
    "++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "Dimensionality Reduction is not just useful for visualization of\n",
    "high dimensional datasets. It can also be used as a preprocessing\n",
    "step (often called data normalization) to help speed up supervised\n",
    "machine learning methods that are not computationally efficient with high\n",
    "``n_features`` such as SVM classifiers with gaussian kernels for\n",
    "instance or that do not work well with linearly correlated features.\n",
    "\n",
    ".. note::\n",
    "\n",
    "  ``scikit-learn`` also features an implementation of Independant\n",
    "  Component Analysis (ICA) and several manifold learning methods\n",
    "  (See :ref:`astro_exercise_3`)\n",
    "\n",
    "\n",
    "Clustering\n",
    "~~~~~~~~~~\n",
    "\n",
    "Clustering is the task of gathering samples into groups of similar\n",
    "samples according to some predefined similarity or dissimilarity\n",
    "measure (such as the Euclidean distance).\n",
    "\n",
    "For example, let us reuse the output of the 2D PCA of the iris\n",
    "dataset and try to find 3 groups of samples using the simplest\n",
    "clustering algorithm (KMeans)::\n",
    "\n",
    "  >>> from sklearn.cluster import KMeans\n",
    "  >>> from numpy.random import RandomState\n",
    "  >>> rng = RandomState(42)\n",
    "\n",
    "  >>> kmeans = KMeans(n_clusters=3, random_state=rng).fit(X_pca)\n",
    "\n",
    "  >>> np.round(kmeans.cluster_centers_, decimals=2)\n",
    "  array([[ 1.02, -0.71],\n",
    "         [ 0.33,  0.89],\n",
    "         [-1.29, -0.44]])\n",
    "\n",
    "  >>> kmeans.labels_[:10]\n",
    "  array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
    "\n",
    "  >>> kmeans.labels_[-10:]\n",
    "  array([0, 0, 1, 0, 0, 0, 1, 0, 0, 1])\n",
    "\n",
    "We can plot the assigned cluster labels instead of the target names\n",
    "with::\n",
    "\n",
    "   plot_2D(X_pca, kmeans.labels_, [\"c0\", \"c1\", \"c2\"])\n",
    "\n",
    "\n",
    "\n",
    ".. figure:: auto_examples/images/plot_iris_projections_2.png\n",
    "   :target: auto_examples/plot_iris_projections.html\n",
    "   :scale: 65 %\n",
    "   :align: center\n",
    "   :alt: KMeans cluster assignements on 2D PCA iris data\n",
    "\n",
    "   KMeans cluster assignements on 2D PCA iris data\n",
    "\n",
    ".. topic:: **Exercise**\n",
    "   :class: green\n",
    "\n",
    "   Repeat the clustering algorithm from above, but fit the clusters to\n",
    "   the full dataset ``X`` rather than the projection ``X_pca``.  Do the\n",
    "   labels computed this way better match the true labels?\n",
    "\n",
    "\n",
    "Notable implementations of clustering models\n",
    "++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "The following are two well-known clustering algorithms. Like most\n",
    "unsupervised learning models in the scikit, they expect the data\n",
    "to be clustered to have the shape ``(n_samples, n_features)``:\n",
    "\n",
    ":class:`sklearn.cluster.KMeans`\n",
    "\n",
    "  The simplest, yet effective clustering algorithm. Needs to be\n",
    "  provided with the number of clusters in advance, and assumes that the\n",
    "  data is normalized as input (but use a PCA model as preprocessor).\n",
    "\n",
    ":class:`sklearn.cluster.MeanShift`\n",
    "\n",
    "  Can find better looking clusters than KMeans but is not scalable\n",
    "  to high number of samples.\n",
    "\n",
    ":class:`sklearn.cluster.DBSCAN`\n",
    "  Can detect irregularly shaped clusters based on density, i.e. sparse regions\n",
    "  in the input space are likely to become inter-cluster boundaries. Can also\n",
    "  detect outliers (samples that are not part of a cluster).\n",
    "\n",
    ":class:`sklearn.manifold.LocallyLinearEmbedding`\n",
    "\n",
    "  Locally Linear Embedding is a nonlinear neighbors-based\n",
    "  manifold learning technique.\n",
    "  The scikit-learn implementation makes available several variants to\n",
    "  the basic algorithm.\n",
    "\n",
    ":class:`sklearn.manifold.Isomap`\n",
    "\n",
    "  Isomap is another neighbors-based manifold learning method that can find\n",
    "  nonlinear projections of data.\n",
    "\n",
    "Other clustering algorithms do not work with a data array of shape\n",
    "``(n_samples, n_features)`` but directly with a precomputed affinity matrix\n",
    "of shape ``(n_samples, n_samples)``:\n",
    "\n",
    ":class:`sklearn.cluster.AffinityPropagation`\n",
    "\n",
    "  Clustering algorithm based on message passing between data points.\n",
    "\n",
    ":class:`sklearn.cluster.SpectralClustering`\n",
    "\n",
    "  KMeans applied to a projection of the normalized graph Laplacian:\n",
    "  finds normalized graph cuts if the affinity matrix is interpreted\n",
    "  as an adjacency matrix of a graph.\n",
    "\n",
    ":class:`sklearn.cluster.Ward`\n",
    "\n",
    "  ``Ward`` implements hierarchical clustering based on the Ward algorithm,\n",
    "     a variance-minimizing approach. At each step, it minimizes the sum of\n",
    "     squared differences within all clusters (inertia criterion).\n",
    "\n",
    "``DBSCAN`` can work with either an array of samples or an affinity matrix.\n",
    "\n",
    "\n",
    "Applications of clustering\n",
    "++++++++++++++++++++++++++\n",
    "\n",
    "Here are some common applications of clustering algorithms:\n",
    "\n",
    "- Building customer profiles for market analysis\n",
    "\n",
    "- Grouping related web news (e.g. Google News) and websearch results\n",
    "\n",
    "- Grouping related stock quotes for investment portfolio management\n",
    "\n",
    "- Can be used as a preprocessing step for recommender systems\n",
    "\n",
    "- Can be used to build a code book of prototype samples for unsupervised\n",
    "  feature extraction for supervised learning algorithms\n",
    "\n",
    "\n",
    "Linearly separable data\n",
    "-----------------------\n",
    "\n",
    "Some supervised learning problems can be solved by very simple\n",
    "models (called generalized linear models) depending on the data.\n",
    "Others simply don't.\n",
    "\n",
    "To grasp the difference between the two cases, run the interactive\n",
    "example from the ``examples`` folder of the ``scikit-learn`` source\n",
    "distribution.  (if you don't have the scikit-learn source code locally\n",
    "installed, you can find the script `here <auto_examples/svm_gui.html>`_)::\n",
    "\n",
    "    % python $SKL_HOME/examples/svm_gui.py\n",
    "\n",
    "1. Put some data points belonging to one of the two target classes\n",
    "   ('white' or 'black') using left click and right click.\n",
    "\n",
    "2. Choose some parameters of a Support Vector Machine to be trained on\n",
    "   this toy dataset (``n_samples`` is the number of clicks, ``n_features``\n",
    "   is 2).\n",
    "\n",
    "3. Click the Fit but to train the model and see the decision boundary.\n",
    "   The accurracy of the model is displayed on stdout.\n",
    "\n",
    "The following figures demonstrate one case where a linear model can\n",
    "perfectly separate the two classes while the other is not linearly\n",
    "separable (a model with a gaussian kernel is required in that case).\n",
    "\n",
    "\n",
    ".. figure:: auto_examples/images/plot_gui_example_1.png\n",
    "   :target: auto_examples/plot_gui_example.html\n",
    "   :scale: 65 %\n",
    "   :align: center\n",
    "   :alt: Example of a linear SVM fit\n",
    "\n",
    "   Linear Support Vector Machine trained to perfectly separate 2 sets of\n",
    "   data points labeled as white and black in a 2D space.\n",
    "\n",
    "\n",
    ".. figure:: auto_examples/images/plot_gui_example_2.png\n",
    "   :target: auto_examples/plot_gui_example.html\n",
    "   :scale: 65 %\n",
    "   :align: center\n",
    "   :alt: Example of a gaussian SVM fit\n",
    "\n",
    "   Support Vector Machine with gaussian kernel trained to separate 2 sets of\n",
    "   data points labeled as white and black in a 2D space. This dataset would\n",
    "   not have been seperated by a simple linear model.\n",
    "\n",
    ".. topic:: **Exercise**\n",
    "   :class: green\n",
    "\n",
    "   Fit a model that is able to solve the XOR problem using the GUI:\n",
    "   the XOR problem is composed of 4 samples:\n",
    "\n",
    "     - 2 white samples in the top-left and bottom-right corners\n",
    "\n",
    "     - 2 black samples in the bottom-left and top-right corners\n",
    "\n",
    "   **Question**: is the XOR problem linearly separable?\n",
    "\n",
    ".. topic:: **Exercise**\n",
    "   :class: green\n",
    "\n",
    "   Construct a problem with less than 10 points where the predictive\n",
    "   accuracy of the best linear model is 50%.\n",
    "\n",
    ".. note::\n",
    "\n",
    "  the higher the dimension of the feature space, the more likely\n",
    "  the data is linearly separable: for instance this is often the\n",
    "  case for text classification tasks.\n",
    "\n",
    "\n",
    "Hyperparameters, training set, test set and overfitting\n",
    "-------------------------------------------------------\n",
    "\n",
    "The above SVM example displays an example of *hyperparameters*, which are\n",
    "model parameters set before the training process.  For example, when using\n",
    "an RBF model, we choose the kernel coefficient ``gamma`` before fitting the\n",
    "data. We must be able to then evaluate the goodness-of-fit of our model\n",
    "given this choice of hyperparameter.\n",
    "\n",
    "The most common mistake beginners make when training statistical\n",
    "models is to evaluate the quality of the model on the same data\n",
    "used for fitting the model:\n",
    "\n",
    "  If you do this, **you are doing it wrong!**\n",
    "\n",
    "\n",
    "The overfitting issue\n",
    "~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Evaluating the quality of the model on the data used to fit the model can\n",
    "lead to *overfitting*.  Consider the following dataset, and three fits to\n",
    "the data (we'll explore this example in more detail in the\n",
    ":ref:`next section <astro_biasvariance>`).\n",
    "\n",
    ".. figure:: auto_examples/images/plot_bias_variance_examples_2.png\n",
    "   :target: auto_examples/plot_bias_variance_examples.html\n",
    "   :align: center\n",
    "   :scale: 80%\n",
    "\n",
    "   Examples of over-fitting and under-fitting a two-dimensional dataset.\n",
    "\n",
    "Evaluating the :math:`d=6` model using the training data might lead you to\n",
    "believe the model is very good, when in fact it does not do a good job of\n",
    "representing the data.\n",
    "The problem lies in the fact that some models can be subject to the\n",
    "**overfitting** issue: they can **learn the training data by heart**\n",
    "without generalizing. The symptoms are:\n",
    "\n",
    "  - the predictive accurracy on the data used for training can be excellent\n",
    "    (sometimes 100%)\n",
    "\n",
    "  - however, the models do little better than random prediction when facing\n",
    "    new data that was not part of the training set\n",
    "\n",
    "If you evaluate your model on your training data you won't be able to tell\n",
    "whether your model is overfitting or not.\n",
    "\n",
    "\n",
    "Solutions to overfitting\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The solution to this issue is twofold:\n",
    "\n",
    "  1. Split your data into two sets to detect overfitting situations:\n",
    "\n",
    "    - one for training and model selection: the **training set**\n",
    "\n",
    "    - one for evaluation: the **test set**\n",
    "\n",
    "  2. Avoid overfitting by using simpler models (e.g. linear classifiers\n",
    "     instead of gaussian kernel SVM) or by increasing the regularization\n",
    "     parameter of the model if available (see the docstring of the\n",
    "     model for details)\n",
    "\n",
    "An even better option when experimenting with classifiers is to divide\n",
    "the data into three sets: training, testing and holdout. You can then\n",
    "optimize your features, settings and algorithms for the testing set until\n",
    "they seem good enough, and finally test on the holdout set (perhaps after\n",
    "adding the test set to the training set).\n",
    "\n",
    "When the amount of labeled data available is small, it may not be feasible\n",
    "to construct training and test sets. In that case, you can choose to\n",
    "use **k-fold cross validation**:\n",
    "divide the dataset into `k` = 10 parts of (roughly) equal size, then for\n",
    "each of these ten parts, train the classifier on the other nine and test\n",
    "on the held-out part.\n",
    "\n",
    "\n",
    "Measuring classification performance on a test set\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    ".. note::\n",
    "\n",
    "   The information in this section is available in an interactive notebook\n",
    "   :download:`05_iris_crossval.ipynb <notebooks/05_iris_crossval.ipynb>`,\n",
    "   which can be viewed using `iPython notebook`_.    An online static view can\n",
    "   be seen `here <http://nbviewer.ipython.org/url/astroml.github.com/sklearn_tutorial/_downloads/05_iris_crossval.ipynb>`_\n",
    "\n",
    "Here is an example on you to split the data on the iris dataset.\n",
    "\n",
    "First we need to shuffle the order of the samples and the target\n",
    "to ensure that all classes are well represented on both sides of\n",
    "the split::\n",
    "\n",
    "  >>> indices = np.arange(n_samples)\n",
    "  >>> indices[:10]\n",
    "  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "  >>> RandomState(42).shuffle(indices)\n",
    "  >>> indices[:10]\n",
    "  array([ 73,  18, 118,  78,  76,  31,  64, 141,  68,  82])\n",
    "\n",
    "  >>> X = iris.data[indices]\n",
    "  >>> y = iris.target[indices]\n",
    "\n",
    "We can now split the data using a 2/3 - 1/3 ratio::\n",
    "\n",
    "  >>> split = (n_samples * 2) / 3\n",
    "\n",
    "  >>> X_train, X_test = X[:split], X[split:]\n",
    "  >>> y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "  >>> X_train.shape\n",
    "  (100, 4)\n",
    "\n",
    "  >>> X_test.shape\n",
    "  (50, 4)\n",
    "\n",
    "  >>> y_train.shape\n",
    "  (100,)\n",
    "\n",
    "  >>> y_test.shape\n",
    "  (50,)\n",
    "\n",
    "We can now re-train a new linear classifier on the training set only::\n",
    "\n",
    "  >>> clf = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "To evaluate its quality we can compute the average number of correct\n",
    "classifications on the test set::\n",
    "\n",
    "  >>> np.mean(clf.predict(X_test) == y_test)\n",
    "  1.0\n",
    "\n",
    "This shows that the model has a predictive accurracy of 100% which\n",
    "means that the classification model was perfectly capable of\n",
    "generalizing what was learned from the training set to the test\n",
    "set: this is rarely so easy on real life datasets as we will see\n",
    "in the following chapter.\n",
    "\n",
    "In the :ref:`next section <astro_biasvariance>`, we will explore in more\n",
    "detail the bias-variance tradeoff and the practical use of machine learning\n",
    "techniques.\n",
    "\n",
    "\n",
    "Key takeaway points\n",
    "-------------------\n",
    "\n",
    "- Build ``X`` (features vectors) with shape ``(n_samples, n_features)``\n",
    "\n",
    "- Supervised learning: ``clf.fit(X, y)`` and then ``clf.predict(X_new)``\n",
    "\n",
    "  - Classification: ``y`` is an array of integers\n",
    "\n",
    "  - Regression: ``y`` is an array of floats\n",
    "\n",
    "- Unsupervised learning: ``clf.fit(X)``\n",
    "\n",
    "  - Dimensionality Reduction with ``clf.transform(X_new)``\n",
    "\n",
    "    - for visualization\n",
    "\n",
    "    - for scalability\n",
    "\n",
    "  - Clustering finds group id for each sample\n",
    "\n",
    "- Some models work much better with data normalized with PCA\n",
    "\n",
    "- Simple linear models can fail completely (non linearly separable data)\n",
    "\n",
    "- Simple linear models often very useful in practice (esp. with\n",
    "  large ``n_features``)\n",
    "\n",
    "- Before starting to train a model: split train / test data:\n",
    "\n",
    "  - use training set for model selection and fitting\n",
    "\n",
    "  - use test set for model evaluation\n",
    "\n",
    "  - use cross-validation when your dataset is small\n",
    "\n",
    "- Complex models can overfit (learn by heart) the training data and\n",
    "  fail to generalize correctly on test data:\n",
    "\n",
    "  - try simpler models first\n",
    "\n",
    "  - tune the regularization parameter on a validation set\n",
    "\n",
    "\n",
    ".. _`iPython notebook`: http://ipython.org/ipython-doc/stable/interactive/htmlnotebook.html\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}